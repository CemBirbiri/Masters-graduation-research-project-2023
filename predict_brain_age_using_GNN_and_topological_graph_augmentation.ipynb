{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bd252d3c",
        "2a7497a9",
        "33aa34cb",
        "aad997e1",
        "ecaaf4b5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5U4EE6K86v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056c17c1-6a87-4f65-f7db-be48c62d048d"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dfbb72"
      },
      "source": [
        "# 1. Load Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zASDWgn17m5v",
        "outputId": "745a958d-0ccd-4e4a-fbc4-b83c308dbc8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd252d3c"
      },
      "source": [
        "## 1.1 Load Target values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fa6b12a"
      },
      "source": [
        "In the target-dataset, we have 1206 patients(1206 rows in the dataset) and 583 different targets(583 different target values). If we build a machine learning model, we can predict these 583 different targets. The target selection is based on us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ae0d63"
      },
      "outputs": [],
      "source": [
        "#Load the target file to Jupyter Notebook\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_struct = pd.read_csv('/content/drive/MyDrive/GUDHI-tda-tutorials-mapper/df_struct.csv')\n",
        "print(df_struct.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08e711e9"
      },
      "outputs": [],
      "source": [
        "#Let's see what kind of targets we have:\n",
        "df_struct.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a7497a9"
      },
      "source": [
        "## 1.2 Load DTI connectivity matrices of 998 patients:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b404c9ba"
      },
      "source": [
        "Our goal is to predict targets given by connectivity matrices. These connectivity matrices represent neural relationships of different brain regions. We have 998 connectivity matrices so we have 998 patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef92bbf3"
      },
      "outputs": [],
      "source": [
        "# Let's now load it into a list of 998 pandas DataFrames\n",
        "import pickle\n",
        "open_file = open('/content/drive/MyDrive/GUDHI-tda-tutorials-mapper/DTI.txt', \"rb\")\n",
        "loaded_DTI = pickle.load(open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1a7178"
      },
      "outputs": [],
      "source": [
        "#Just to check that we 998 connectivity matrixes\n",
        "len(loaded_DTI), type(loaded_DTI)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_DTI[0].shape"
      ],
      "metadata": {
        "id": "2JUurdAllcNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1388716"
      },
      "source": [
        "The connectivity matrices have shape (116,116). That means we have 116 brain regions."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U74k_JJ2luxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33aa34cb"
      },
      "source": [
        "## 1.3 Take tranpose to make matrices symmetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf6f7bf"
      },
      "source": [
        "Our connectivity matrices are not symmetric. The DTI_ab matrix measures the amount of fibers from a to b and DTI_ba from b to a, which may differ. Simply, we have a directed graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005954a5"
      },
      "outputs": [],
      "source": [
        "DTI_sym = loaded_DTI.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_DTI[0]"
      ],
      "metadata": {
        "id": "P3rM_ASm_TSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2502ff86"
      },
      "outputs": [],
      "source": [
        "#Make the matrices symmetric\n",
        "\n",
        "for i in range(len(loaded_DTI)):\n",
        "    DTI_sym[i] = (DTI_sym[i] + DTI_sym[i].T)/2\n",
        "DTI_sym[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a3dff12"
      },
      "outputs": [],
      "source": [
        "len(DTI_sym), type(DTI_sym), type(DTI_sym[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad997e1"
      },
      "source": [
        "## 1.4 Load IDs of 998 patient:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fcb0c8"
      },
      "source": [
        "In the target-dataset we had 1206 patients, however we have 998 connectivity matrices. So we need to select 998 patients' target values. How to do it? By matching the IDs of patients. First we load the patient IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "818a9517"
      },
      "outputs": [],
      "source": [
        "# You may need the IDs of all DTI networks in your project. We give this for you here:\n",
        "#!wget -O 'IDsDTI.txt' https://www.dropbox.com/s/k7wuffrr0e26a7m/IDsDTI.txt?dl=0\n",
        "open_file = open('/content/drive/MyDrive/GUDHI-tda-tutorials-mapper/IDsDTI.txt', \"rb\")\n",
        "DTI_IDs = pickle.load(open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aecf2f9"
      },
      "outputs": [],
      "source": [
        "len(DTI_IDs), type(DTI_IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2da194d7"
      },
      "outputs": [],
      "source": [
        "DTI_IDs = pd.DataFrame(DTI_IDs,columns =['Subject'])\n",
        "DTI_IDs.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1LZs_pJl_GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecaaf4b5"
      },
      "source": [
        "## 1.5 Match ID's with target values:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54bb4fd1"
      },
      "source": [
        "We do mathing the patient IDs with the target values here. The 'Subject' column in both datasets represent the IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5042e2b"
      },
      "outputs": [],
      "source": [
        "df_targets = df_struct.copy()\n",
        "\n",
        "for i in range(len(df_targets)):\n",
        "    flag=0\n",
        "    for j in range(len(DTI_IDs)):\n",
        "        if int(DTI_IDs['Subject'][j])== df_targets['Subject'][i]:\n",
        "            flag = 1\n",
        "    if flag==0:\n",
        "        df_targets = df_targets.drop([i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffab492f"
      },
      "outputs": [],
      "source": [
        "df_targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.6 Exract AGE column"
      ],
      "metadata": {
        "id": "FXTDvc2nmTT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_col  = df_targets[\"Age\"].copy()\n",
        "age_col_np = age_col.to_numpy()\n",
        "\n",
        "ages=[]\n",
        "#Group the age categories:\n",
        "for i in range(len(age_col)):\n",
        "  if age_col_np[i]== '22-25':\n",
        "    ages.append(0)\n",
        "  elif age_col_np[i]== '26-30':\n",
        "    ages.append(1)\n",
        "  elif age_col_np[i]== '31-35':\n",
        "    ages.append(2)\n",
        "  elif age_col_np[i]== '36+':\n",
        "    ages.append(3)\n"
      ],
      "metadata": {
        "id": "BNN46fwQdlFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Create torch.geometric Data graphs (Raw)"
      ],
      "metadata": {
        "id": "7XWPqJuV6v_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "import torch\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.testing import withPackage\n",
        "from torch_geometric.utils import (\n",
        "    from_networkx,\n",
        "    from_scipy_sparse_matrix,\n",
        "    from_trimesh,\n",
        "    subgraph,\n",
        "    to_networkx,\n",
        "    to_scipy_sparse_matrix,\n",
        "    to_trimesh,\n",
        ")\n",
        "def test_from_networkx():\n",
        "    x = torch.randn(2, 8)\n",
        "    print(\"x = \", x)\n",
        "    pos = torch.randn(2, 3)\n",
        "    edge_index = torch.tensor([[0, 1, 0], [1, 0, 0]])\n",
        "    edge_attr = torch.tensor([[1,3], [2,3], [3,3]])\n",
        "    #edge_attr = torch.randn(edge_index.size(1))\n",
        "    print(edge_attr)\n",
        "    print(edge_attr.shape)\n",
        "    print(type(edge_attr))\n",
        "    perm = torch.tensor([0, 2, 1])\n",
        "    data = Data(x=x, pos=pos, edge_index=edge_index, edge_attr=edge_attr)\n",
        "    G = to_networkx(data, node_attrs=['x', 'pos'], edge_attrs=['edge_attr'])\n",
        "    data = from_networkx(G)\n",
        "    print(data)\n",
        "    assert len(data) == 4\n",
        "    assert data.x.tolist() == x.tolist()\n",
        "    assert data.pos.tolist() == pos.tolist()\n",
        "    assert data.edge_index.tolist() == edge_index[:, perm].tolist()\n",
        "    assert data.edge_attr.tolist() == edge_attr[perm].tolist()\n",
        "\n",
        "#test_from_networkx()"
      ],
      "metadata": {
        "id": "xJmvggAEshgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Visualize graphs"
      ],
      "metadata": {
        "id": "j9Qj3bkIkSHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from pylab import rcParams\n",
        "import networkx as nx\n",
        "\n",
        "df_empty = pd.DataFrame({'source': [], 'target':[], 'weight' : []})\n",
        "DTI_sym_np_temp = DTI_sym[990].to_numpy()\n",
        "columns = DTI_sym[990].columns\n",
        "\n",
        "for i in range(0,116):\n",
        "  for j in range(i,116):\n",
        "    if DTI_sym_np_temp[i][j] != 0.:\n",
        "      new_row = {'source': columns[i], 'target': columns[j], 'weight': DTI_sym_np_temp[i][j] }\n",
        "      df_empty = df_empty.append(new_row, ignore_index=True)\n",
        "\n",
        "G = nx.from_pandas_edgelist(df_empty, 'source', 'target', [\"weight\"])\n",
        "color_lookup = {k:v for v, k in enumerate(sorted(set(G.nodes())))}\n",
        "# {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'J': 6, 'K': 7, 'Z': 8}\n",
        "\n",
        "low, *_, high = sorted(color_lookup.values())\n",
        "norm = mpl.colors.Normalize(vmin=low, vmax=high, clip=True)\n",
        "mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
        "\n",
        "rcParams['figure.figsize'] = 9, 6\n",
        "nx.draw(G,\n",
        "        nodelist=color_lookup,\n",
        "        node_size=670,\n",
        "        node_color=[mapper.to_rgba(i)\n",
        "                    for i in color_lookup.values()],\n",
        "        with_labels=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PFbS6OxXjL3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_in(first, second, already_looked):\n",
        "  if already_looked==[]:\n",
        "    return 0\n",
        "  else:\n",
        "    for i in range(len(already_looked)):\n",
        "      if already_looked[i][0] == first and already_looked[i][1] == second:\n",
        "        return 1\n",
        "    return 0\n",
        "\n"
      ],
      "metadata": {
        "id": "t1_KPuITsPLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
        "import torch\n",
        "import scipy.sparse\n",
        "import random\n",
        "import torch\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.testing import withPackage\n",
        "from torch_geometric.utils import (\n",
        "    from_networkx,\n",
        "    from_scipy_sparse_matrix,\n",
        "    from_trimesh,\n",
        "    subgraph,\n",
        "    to_networkx,\n",
        "    to_scipy_sparse_matrix,\n",
        "    to_trimesh,\n",
        ")\n",
        "\n",
        "DTI_torch_geometric_graphs=[]\n",
        "for dti in range( 0,500):#len(DTI_sym)):\n",
        "  print(dti)\n",
        "\n",
        "  DTI_sym_np = DTI_sym[dti].to_numpy()\n",
        "  G = nx.from_numpy_array(DTI_sym_np) #Bu CALISIYOR!!\n",
        "\n",
        "  #edge_index'i kendi hesapliyor zaten.Önce torch.geometric grapah cevirip ordan Torch.tensor type'ında edge_index'i kaydediyoruz\n",
        "\n",
        "\n",
        "  #Node attribute'ü ayarla:\n",
        "  eigen = nx.eigenvector_centrality(G, weight='weight',tol=1e-03)\n",
        "  #betweenness = nx.betweenness_centrality(G, weight='distance', normalized=True)\n",
        "  closeness = nx.closeness_centrality(G)\n",
        "\n",
        "\n",
        "  random_number = random.randint(1, 100)\n",
        "\n",
        "  node_attributes=[]\n",
        "  for i in range(len(eigen)):\n",
        "\n",
        "    #node_attributes.append( [ closeness[i]])     #eigen[i]#, betweenness[i], closeness[i]] )\n",
        "    node_attributes.append( [ random_number * 1.0] )\n",
        "  x = torch.tensor(node_attributes)\n",
        "\n",
        "\n",
        "  #Edge attribute'ü ayarla\n",
        "  G_distance_dict = {(e1, e2): 1 / abs(weight) for e1, e2, weight in G.edges(data='weight')}\n",
        "  edge_attr0=[]\n",
        "  edge_indexes = [[],[]]\n",
        "  for (key, value)in G_distance_dict.items():\n",
        "    edge_attr0.append(value)\n",
        "    #edge_attr.append([value])\n",
        "    edge_indexes[0].append(key[0])\n",
        "    edge_indexes[1].append(key[1])\n",
        "\n",
        "\n",
        "  edge_indexes = torch.LongTensor(edge_indexes)\n",
        "  edge_attr = [float(i)/sum(edge_attr0) for i in edge_attr0]\n",
        "\n",
        "  '''\n",
        "  #Edge index'i ayarla\n",
        "  pyg_graph = from_networkx(G)\n",
        "  #edge_index = pyg_graph.edge_index\n",
        "\n",
        "  index =0\n",
        "  already_looked=[(0,0)]\n",
        "  edge_index_tmp = torch.empty((2, int(pyg_graph.edge_index.shape[1]/2)))\n",
        "  for i in range( int(pyg_graph.edge_index.shape[1]) ):\n",
        "    if is_in(pyg_graph.edge_index[0][i], pyg_graph.edge_index[1][i], already_looked) == 0:\n",
        "      edge_index_tmp[0][index] = int(pyg_graph.edge_index[0][i])\n",
        "      edge_index_tmp[1][index] = int(pyg_graph.edge_index[1][i])\n",
        "      already_looked.append( (pyg_graph.edge_index[1][i], pyg_graph.edge_index[0][i]) )\n",
        "      already_looked.append( (pyg_graph.edge_index[0][i], pyg_graph.edge_index[1][i]) )\n",
        "      index +=1\n",
        "  '''\n",
        "  #Data objesini hazırla\n",
        "  data = Data(x=x,  edge_index=edge_indexes, edge_attr=torch.Tensor(edge_attr), y = torch.LongTensor([ages[dti]]) )\n",
        "\n",
        "\n",
        "  DTI_torch_geometric_graphs.append(data)\n",
        "\n",
        "DTI_torch_geometric_graphs[0]"
      ],
      "metadata": {
        "id": "9WWQjyevLiVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Build GNN model (raw)"
      ],
      "metadata": {
        "id": "ms5PlguwfOih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = DTI_torch_geometric_graphs[0]\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "#print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "vXsgXS8tXY4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(12345)\n",
        "#dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = DTI_torch_geometric_graphs[:400]\n",
        "test_dataset = DTI_torch_geometric_graphs[400:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "#for step, data in enumerate(train_loader):\n",
        "    #print(f'Step {step + 1}:')\n",
        "    #print('=======')\n",
        "    #print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    #print(data)\n",
        "    #print()"
      ],
      "metadata": {
        "id": "zrVP4RL3fQ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#USE edge attr\n",
        "from torch.nn import Linear, Softmax\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, TransformerConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_channels, conv_params={}):\n",
        "        super(GNN, self).__init__()\n",
        "        #torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(input_size, hidden_channels, **conv_params)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels, **conv_params)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels, **conv_params)\n",
        "        self.lin = Linear(hidden_channels, 4)\n",
        "\n",
        "    def forward(self, x, edge_index, batch,  edge_col = None):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index, edge_col)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index, edge_col)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index, edge_col)\n",
        "        # 2. Readout layer\n",
        "        batch = torch.zeros(data.x.shape[0],dtype=int) if batch is None else batch\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "model = GNN(input_size = 1, hidden_channels=64)"
      ],
      "metadata": {
        "id": "pQ8Fxs9gkqOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#NOT USE edge attr\n",
        "\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, NNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(1, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 4)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=8)\n",
        "print(model)\n",
        "'''"
      ],
      "metadata": {
        "id": "iZeg8SAyijBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "#model = GCN(hidden_channels=8)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    losss=0\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "\n",
        "         #Not use edge attr\n",
        "         #out = model(data.x, data.edge_index, data.batch)\n",
        "         #Use edge attr\n",
        "         out = model(data.x, data.edge_index, data.batch, data.edge_attr)  # Perform a single forward pass.\n",
        "\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         losss+=loss\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "    print(losss)\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         #out = model(data.x, data.edge_index, data.batch)\n",
        "         out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
        "         #print(\"out=\",out)\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         #print(\"pred=\",pred)\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "         #print(\"correct = \",correct)\n",
        "\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "IWIu5FU_i-g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Create Topological graphs"
      ],
      "metadata": {
        "id": "Hw6AW9lAF7Mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 cover_complex.py (helper functions)\n"
      ],
      "metadata": {
        "id": "JKQfKNUu_WZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.base            import BaseEstimator, TransformerMixin\n",
        "from sklearn.cluster         import DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics         import pairwise_distances\n",
        "from scipy.spatial.distance  import directed_hausdorff"
      ],
      "metadata": {
        "id": "1ArH0VPJ9nIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoverComplexPy(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This is a mother class for MapperComplex, GraphInducedComplex and NerveComplex.\n",
        "    Attributes:\n",
        "        simplex_tree (gudhi SimplexTree): simplicial complex representing the cover complex computed after calling the fit() method.\n",
        "        node_info (dictionary): various information associated to the nodes of the cover complex.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_name=\"data\", cover_name=\"cover\", color_name=\"color\", verbose=False):\n",
        "        \"\"\"\n",
        "        Constructor for the CoverComplexPy class.\n",
        "        Parameters:\n",
        "        \"\"\"\n",
        "        self.input_name, self.cover_name, self.color_name, self.verbose = input_name, cover_name, color_name, verbose\n",
        "\n",
        "    def get_networkx(self, get_attrs=False):\n",
        "        \"\"\"\n",
        "        Turn the 1-skeleton of the cover complex computed after calling fit() method into a networkx graph.\n",
        "        This function requires networkx (https://networkx.org/documentation/stable/install.html).\n",
        "        Parameters:\n",
        "            get_attrs (bool): if True, the color functions will be used as attributes for the networkx graph.\n",
        "        Returns:\n",
        "            G (networkx graph): graph representing the 1-skeleton of the cover complex.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import networkx as nx\n",
        "            st = self.simplex_tree\n",
        "            G = nx.Graph()\n",
        "            for (splx,_) in st.get_skeleton(1):\n",
        "                if len(splx) == 1:\n",
        "                    G.add_node(splx[0])\n",
        "                if len(splx) == 2:\n",
        "                    G.add_edge(splx[0], splx[1])\n",
        "            if get_attrs:\n",
        "                attrs = {k: {\"attr_name\": self.node_info[k][\"colors\"]} for k in G.nodes()}\n",
        "                nx.set_node_attributes(G, attrs)\n",
        "            return G\n",
        "        except ImportError:\n",
        "            print(\"Networkx not found, nx graph not computed\")\n",
        "\n",
        "    def print_to_dot(self, epsv=.2, epss=.4):\n",
        "        \"\"\"\n",
        "        Write the cover complex in a DOT file called \"{self.input_name}.dot\", that can be processed with, e.g., neato.\n",
        "        Parameters:\n",
        "            epsv (float): scale the node colors between [epsv, 1-epsv]\n",
        "            epss (float): scale the node sizes between [epss, 1-epss]\n",
        "        \"\"\"\n",
        "        st = self.simplex_tree\n",
        "        node_info = self.node_info\n",
        "\n",
        "        maxv, minv = max([node_info[k][\"colors\"][0] for k in node_info.keys()]), min([node_info[k][\"colors\"][0] for k in node_info.keys()])\n",
        "        maxs, mins = max([node_info[k][\"size\"]      for k in node_info.keys()]), min([node_info[k][\"size\"]      for k in node_info.keys()])\n",
        "\n",
        "        f = open(self.input_name + \".dot\", \"w\")\n",
        "        f.write(\"graph MAP{\")\n",
        "        cols = []\n",
        "        for (simplex,_) in st.get_skeleton(0):\n",
        "            cnode = (1.-2*epsv) * (node_info[simplex[0]][\"colors\"][0] - minv)/(maxv-minv) + epsv if maxv != minv else 0\n",
        "            snode = (1.-2*epss) * (node_info[simplex[0]][\"size\"]-mins)/(maxs-mins) + epss if maxs != mins else 1\n",
        "            f.write(  str(simplex[0]) + \"[shape=circle width=\" + str(snode) + \" fontcolor=black color=black label=\\\"\"  + \"\\\" style=filled fillcolor=\\\"\" + str(cnode) + \", 1, 1\\\"]\")\n",
        "            cols.append(cnode)\n",
        "        for (simplex,_) in st.get_filtration():\n",
        "            if len(simplex) == 2:\n",
        "                f.write(\"  \" + str(simplex[0]) + \" -- \" + str(simplex[1]) + \" [weight=15];\")\n",
        "        f.write(\"}\")\n",
        "        f.close()\n",
        "\n",
        "        L = np.linspace(epsv, 1.-epsv, 100)\n",
        "        colsrgb = []\n",
        "        try:\n",
        "            import colorsys\n",
        "            for c in L:\n",
        "                colsrgb.append(colorsys.hsv_to_rgb(c,1,1))\n",
        "            fig, ax = plt.subplots(figsize=(6, 1))\n",
        "            fig.subplots_adjust(bottom=0.5)\n",
        "            my_cmap = matplotlib.colors.ListedColormap(colsrgb, name=self.color_name)\n",
        "            cb = matplotlib.colorbar.ColorbarBase(ax, cmap=my_cmap, norm=matplotlib.colors.Normalize(vmin=minv, vmax=maxv), orientation=\"horizontal\")\n",
        "            cb.set_label(self.color_name)\n",
        "            fig.savefig(\"colorbar_\" + self.color_name + \".pdf\", format=\"pdf\")\n",
        "            plt.close()\n",
        "        except ImportError:\n",
        "            print(\"colorsys not found, colorbar not printed\")\n",
        "\n",
        "    def print_to_txt(self):\n",
        "        \"\"\"\n",
        "        Write the cover complex to a TXT file called \"{self.input_name}.txt\", that can be processed with KeplerMapper.\n",
        "        \"\"\"\n",
        "        st = self.simplex_tree\n",
        "        f = open(self.input_name + \".txt\", \"w\")\n",
        "        f.write(self.input_name + \"\\n\")\n",
        "        f.write(self.cover_name + \"\\n\")\n",
        "        f.write(self.color_name + \"\\n\")\n",
        "        f.write(\"0 0\\n\")\n",
        "        f.write(str(st.num_vertices()) + \" \" + str(len(list(st.get_skeleton(1)))-st.num_vertices()) + \"\\n\")\n",
        "        name2id = {}\n",
        "        idv = 0\n",
        "        for s,_ in st.get_skeleton(0):\n",
        "            f.write(str(idv) + \" \" + str(self.node_info[s[0]][\"colors\"][0]) + \" \" + str(self.node_info[s[0]][\"size\"]) + \"\\n\")\n",
        "            name2id[s[0]] = idv\n",
        "            idv += 1\n",
        "        for s,_ in st.get_skeleton(1):\n",
        "            if len(s) == 2:\n",
        "                f.write(str(name2id[s[0]]) + \" \" + str(name2id[s[1]]) + \"\\n\")\n",
        "        f.close()\n",
        "\n",
        "    class _constant_clustering():\n",
        "        def fit_predict(X):\n",
        "            return np.zeros([len(X)], dtype=np.int32)\n",
        "\n",
        "\n",
        "class MapperComplex(CoverComplexPy, BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This is a class for computing Mapper simplicial complexes on point clouds or distance matrices.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_type=\"point cloud\", colors=None, mask=0,\n",
        "                       filters=None, filter_bnds=None, resolutions=None, gains=None, clustering=DBSCAN(), N=100, beta=0., C=10.,\n",
        "                       input_name=\"data\", cover_name=\"cover\", color_name=\"color\", verbose=False):\n",
        "        \"\"\"\n",
        "        Constructor for the MapperComplex class.\n",
        "        Parameters:\n",
        "            input_type (string): type of input data. Either \"point cloud\" or \"distance matrix\".\n",
        "            colors (numpy array of shape (num_points) x (num_colors)): functions used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of these functions on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.\n",
        "            mask (int): threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **mask** points will be removed.\n",
        "            filters (numpy array of shape (num_points) x (num_filters)): filter functions (sometimes called lenses) used to compute the cover. Each column of the numpy array defines a scalar function defined on the input points.\n",
        "            filter_bnds (numpy array of shape (num_filters) x 2): limits of each filter, of the form [[f_1^min, f_1^max], ..., [f_n^min, f_n^max]]. If one of the values is numpy.nan, it can be computed from the dataset with the fit() method.\n",
        "            resolutions (numpy array of shape num_filters containing integers): resolution of each filter function, ie number of intervals required to cover each filter image. If None, it is estimated from data.\n",
        "            gains (numpy array of shape num_filters containing doubles in [0,1]): gain of each filter function, ie overlap percentage of the intervals covering each filter image.\n",
        "            clustering (class): clustering class (default sklearn.cluster.DBSCAN()). Common clustering classes can be found in the scikit-learn library (such as AgglomerativeClustering for instance). If None, it is set to hierarchical clustering, with scale estimated from data.\n",
        "            N (int): subsampling iterations (default 100) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            beta (double): exponent parameter (default 0.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            C (double): constant parameter (default 10.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            input_name (string): name of dataset. Used when generating plots.\n",
        "            cover_name (string): name of cover. Used when generating plots.\n",
        "            color_name (string): name of color function. Used when generating plots.\n",
        "            verbose (bool): whether to display info while computing.\n",
        "        \"\"\"\n",
        "        self.filters, self.filter_bnds, self.resolutions, self.gains, self.colors, self.clustering = filters, filter_bnds, resolutions, gains, colors, clustering\n",
        "        self.input_type, self.mask, self.N, self.beta, self.C = input_type, mask, N, beta, C\n",
        "        CoverComplexPy.__init__(self, input_name, cover_name, color_name, verbose)\n",
        "\n",
        "    def estimate_scale(self, X, N=100, inp=\"point cloud\", beta=0., C=10.):\n",
        "        \"\"\"\n",
        "        Compute estimated scale of a point cloud or a distance matrix.\n",
        "        Parameters:\n",
        "            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.\n",
        "            N (int): subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            inp (string): either \"point cloud\" or \"distance matrix\". Type of input data (default \"point cloud\").\n",
        "            beta (double): exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            C (double): constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "        Returns:\n",
        "            delta (double): estimated scale that can be used with eg agglomerative clustering.\n",
        "        \"\"\"\n",
        "        num_pts = X.shape[0]\n",
        "        delta, m = 0., int(  num_pts / np.exp((1+beta) * np.log(np.log(num_pts)/np.log(C)))  )\n",
        "        for _ in range(N):\n",
        "            subpop = np.random.choice(num_pts, size=m, replace=False)\n",
        "            if inp == \"point cloud\":\n",
        "                d, _, _ = directed_hausdorff(X, X[subpop,:])\n",
        "            if inp == \"distance matrix\":\n",
        "                d = np.max(np.min(X[:,subpop], axis=1), axis=0)\n",
        "            delta += d/N\n",
        "        return delta\n",
        "\n",
        "    def get_optimal_parameters_for_agglomerative_clustering(self, X, beta=0., C=10., N=100):\n",
        "        \"\"\"\n",
        "        Compute optimal scale and resolutions for a point cloud or a distance matrix.\n",
        "        Parameters:\n",
        "            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.\n",
        "            beta (double): exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            C (double): constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "            N (int): subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.\n",
        "        Returns:\n",
        "            delta (double): optimal scale that can be used with agglomerative clustering.\n",
        "            resolutions (numpy array of shape (num_filters): optimal resolutions associated to each filter.\n",
        "        \"\"\"\n",
        "        num_filt, delta = self.filters.shape[1], 0\n",
        "        delta = self.estimate_scale(X=X, N=N, inp=self.input_type, C=C, beta=beta)\n",
        "\n",
        "        pairwise = pairwise_distances(X, metric=\"euclidean\") if self.input_type == \"point cloud\" else X\n",
        "        pairs = np.argwhere(pairwise <= delta)\n",
        "        num_pairs = pairs.shape[0]\n",
        "        res = []\n",
        "        for f in range(num_filt):\n",
        "            F = self.filters[:,f]\n",
        "            resf = 0\n",
        "            for p in range(num_pairs):\n",
        "                resf = max(resf, abs(F[pairs[p,0]] - F[pairs[p,1]]))\n",
        "            res.append(resf)\n",
        "\n",
        "        return delta, np.array(res)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the MapperComplex class on a point cloud or a distance matrix: compute the Mapper complex and store it in a simplex tree called simplex_tree.\n",
        "        Parameters:\n",
        "            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.\n",
        "            y (n x 1 array): point labels (unused).\n",
        "        \"\"\"\n",
        "        if self.filters is None:\n",
        "            if self.input_type == \"point cloud\":\n",
        "                self.filters = X[:,0:1]\n",
        "            elif self.input_type == \"distance matrix\":\n",
        "                self.filters = X.max(axis=0)[:,None]\n",
        "        if self.colors is None:\n",
        "            if self.input_type == \"point cloud\":\n",
        "                self.colors = X[:,0:1]\n",
        "            elif self.input_type == \"distance matrix\":\n",
        "                self.colors = X.max(axis=0)[:,None]\n",
        "\n",
        "        num_pts, num_filters = self.filters.shape[0], self.filters.shape[1]\n",
        "\n",
        "        # If some filter limits are unspecified, automatically compute them\n",
        "        if self.filter_bnds is None:\n",
        "            self.filter_bnds = np.hstack([np.min(self.filters, axis=0)[:,np.newaxis], np.max(self.filters, axis=0)[:,np.newaxis]])\n",
        "\n",
        "        # If some resolutions are not specified, automatically compute them\n",
        "        if self.gains is None:\n",
        "            self.gains = .33 * np.ones(num_filters)\n",
        "        if self.resolutions is None or self.clustering is None:\n",
        "            delta, resolutions = self.get_optimal_parameters_for_agglomerative_clustering(X=X, beta=self.beta, C=self.C, N=self.N)\n",
        "            if self.clustering is None:\n",
        "                if self.input_type == \"point cloud\":\n",
        "                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta, affinity=\"euclidean\")\n",
        "                else:\n",
        "                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta, affinity=\"precomputed\")\n",
        "            if self.resolutions is None:\n",
        "                self.resolutions = np.multiply(resolutions, 1./self.gains)\n",
        "                self.resolutions = np.array([int( (self.filter_bnds[ir,1]-self.filter_bnds[ir,0])/r) for ir, r in enumerate(self.resolutions)])\n",
        "\n",
        "        # Initialize attributes\n",
        "        self.simplex_tree, self.node_info = SimplexTree(), {}\n",
        "\n",
        "        if np.all(self.gains < .5):\n",
        "\n",
        "            # Compute which points fall in which patch or patch intersections\n",
        "            interval_inds, intersec_inds = np.empty(self.filters.shape), np.empty(self.filters.shape)\n",
        "            for i in range(num_filters):\n",
        "                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]\n",
        "                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)\n",
        "                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)\n",
        "                intersec_endpoints = []\n",
        "                for j in range(1, len(interval_endpoints)-1):\n",
        "                    intersec_endpoints.append(interval_endpoints[j] - g*l / (2 - 2*g))\n",
        "                    intersec_endpoints.append(interval_endpoints[j] + g*l / (2 - 2*g))\n",
        "                interval_inds[:,i] = np.digitize(f, interval_endpoints)\n",
        "                intersec_inds[:,i] = 0.5 * (np.digitize(f, intersec_endpoints) + 1)\n",
        "\n",
        "            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it\n",
        "            binned_data = {}\n",
        "            for i in range(num_pts):\n",
        "                list_preimage = []\n",
        "                for j in range(num_filters):\n",
        "                    a, b = interval_inds[i,j], intersec_inds[i,j]\n",
        "                    list_preimage.append([a])\n",
        "                    if b == a:\n",
        "                        list_preimage[j].append(a+1)\n",
        "                    if b == a-1:\n",
        "                        list_preimage[j].append(a-1)\n",
        "                list_preimage = list(itertools.product(*list_preimage))\n",
        "                for pre_idx in list_preimage:\n",
        "                    try:\n",
        "                        binned_data[pre_idx].append(i)\n",
        "                    except KeyError:\n",
        "                        binned_data[pre_idx] = [i]\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Compute interval endpoints for each filter\n",
        "            l_int, r_int = [], []\n",
        "            for i in range(num_filters):\n",
        "                L, R = [], []\n",
        "                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]\n",
        "                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)\n",
        "                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)\n",
        "                for j in range(len(interval_endpoints)-1):\n",
        "                    L.append(interval_endpoints[j]   - g*l / (2 - 2*g))\n",
        "                    R.append(interval_endpoints[j+1] + g*l / (2 - 2*g))\n",
        "                l_int.append(L)\n",
        "                r_int.append(R)\n",
        "\n",
        "            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it\n",
        "            binned_data = {}\n",
        "            for i in range(num_pts):\n",
        "                list_preimage = []\n",
        "                for j in range(num_filters):\n",
        "                    fval = self.filters[i,j]\n",
        "                    start, end = int(min(np.argwhere(np.array(r_int[j]) >= fval))), int(max(np.argwhere(np.array(l_int[j]) <= fval)))\n",
        "                    list_preimage.append(list(range(start, end+1)))\n",
        "                list_preimage = list(itertools.product(*list_preimage))\n",
        "                for pre_idx in list_preimage:\n",
        "                    try:\n",
        "                        binned_data[pre_idx].append(i)\n",
        "                    except KeyError:\n",
        "                        binned_data[pre_idx] = [i]\n",
        "\n",
        "        # Initialize the cover map, that takes a point and outputs the clusters to which it belongs\n",
        "        cover, clus_base = [[] for _ in range(num_pts)], 0\n",
        "\n",
        "        # For each patch\n",
        "        for preimage in binned_data:\n",
        "\n",
        "            # Apply clustering on the corresponding subpopulation\n",
        "            idxs = np.array(binned_data[preimage])\n",
        "            if len(idxs) > 1:\n",
        "                clusters = self.clustering.fit_predict(X[idxs,:]) if self.input_type == \"point cloud\" else self.clustering.fit_predict(X[idxs,:][:,idxs])\n",
        "            elif len(idxs) == 1:\n",
        "                clusters = np.array([0])\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Collect various information on each cluster\n",
        "            num_clus_pre = np.max(clusters) + 1\n",
        "            for clus_i in range(num_clus_pre):\n",
        "                node_name = clus_base + clus_i\n",
        "                subpopulation = idxs[clusters == clus_i]\n",
        "                self.node_info[node_name] = {}\n",
        "                self.node_info[node_name][\"indices\"] = subpopulation\n",
        "                self.node_info[node_name][\"size\"] = len(subpopulation)\n",
        "                self.node_info[node_name][\"colors\"] = np.mean(self.colors[subpopulation,:], axis=0)\n",
        "                self.node_info[node_name][\"patch\"] = preimage\n",
        "\n",
        "            # Update the cover map\n",
        "            for pt in range(clusters.shape[0]):\n",
        "                node_name = clus_base + clusters[pt]\n",
        "                if clusters[pt] != -1 and self.node_info[node_name][\"size\"] >= self.mask:\n",
        "                    cover[idxs[pt]].append(node_name)\n",
        "\n",
        "            clus_base += np.max(clusters) + 1\n",
        "\n",
        "        # Insert the simplices of the Mapper complex\n",
        "        for i in range(num_pts):\n",
        "            self.simplex_tree.insert(cover[i])\n",
        "\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "ny7B5Han9UOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gudhi"
      ],
      "metadata": {
        "id": "Sth9Uw96-Gh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c84157bc"
      },
      "outputs": [],
      "source": [
        "#from gudhi.sklearn import MapperComplex, GraphInducedComplex, NerveComplex\n",
        "#from cover_complex import MapperComplex, GraphInducedComplex, NerveComplex\n",
        "#Instead of above, we define the MapperComplex in a cell in this notebook above\n",
        "\n",
        "from gudhi import bottleneck_distance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.csgraph    import dijkstra, shortest_path, connected_components\n",
        "from scipy.stats             import ks_2samp, entropy, ttest_ind\n",
        "\n",
        "def find(i, parents):\n",
        "    if parents[i] == i:\n",
        "        return i\n",
        "    else:\n",
        "        return find(parents[i], parents)\n",
        "\n",
        "def union(i, j, parents, f):\n",
        "    if f[i] <= f[j]:\n",
        "        parents[j] = i\n",
        "    else:\n",
        "        parents[i] = j\n",
        "\n",
        "def compute_topological_features(M, threshold=0.):\n",
        "    \"\"\"\n",
        "    Compute the topological features (connected components, up/down branches, loops) of\n",
        "    the 1-skeleton of the cover complex. Connected components and loops are computed with scipy\n",
        "    functions, and branches are detected with Union-Find and 0-dimensional persistence of the 1-skeleton.\n",
        "    Parameters:\n",
        "        threshold (float): any topological feature whose size is less than this parameter\n",
        "        (relative to the first color function) will be discarded.\n",
        "    Returns:\n",
        "        dgm (list of (dim,(a,b)) tuples): list of feature characteristics. dim is the\n",
        "        topological dimension of the feature (0 for CCs and branches, 1 for loops), a,b are the min and max\n",
        "        of the first color function along the feature.\n",
        "        bnds (list of lists): list of feature points. Each element of this list is the list of point IDs\n",
        "        forming the corresponding feature.\n",
        "    \"\"\"\n",
        "    st = M.simplex_tree\n",
        "    num_nodes = st.num_vertices()\n",
        "    function, namefunc, invnamefunc = {}, {}, {}\n",
        "    nodeID = 0\n",
        "    for (s,_) in st.get_skeleton(0):\n",
        "        namefunc[s[0]] = nodeID\n",
        "        invnamefunc[nodeID] = s[0]\n",
        "        function[s[0]] = M.node_info[s[0]][\"colors\"][0]\n",
        "        nodeID += 1\n",
        "    dgm, bnd = [], []\n",
        "\n",
        "    # connected_components\n",
        "    A = np.zeros([num_nodes, num_nodes])\n",
        "    for (splx,_) in st.get_skeleton(1):\n",
        "        if len(splx) == 2:\n",
        "            A[namefunc[splx[0]], namefunc[splx[1]]] = 1\n",
        "            A[namefunc[splx[1]], namefunc[splx[0]]] = 1\n",
        "    _, ccs = connected_components(A, directed=False)\n",
        "    for ccID in np.unique(ccs):\n",
        "        pts = np.argwhere(ccs == ccID).flatten()\n",
        "        vals = [function[invnamefunc[p]] for p in pts]\n",
        "        if np.abs(min(vals) - max(vals)) >= threshold:\n",
        "            dgm.append((0, (min(vals), max(vals))))\n",
        "            bnd.append([invnamefunc[p] for p in pts])\n",
        "\n",
        "    # loops\n",
        "    G = M.get_networkx()\n",
        "    try:\n",
        "        from networkx import cycle_basis\n",
        "        bndall = cycle_basis(G)\n",
        "        for pts in bndall:\n",
        "            vals = [function[p] for p in pts]\n",
        "            if np.abs(min(vals) - max(vals)) >= threshold:\n",
        "                dgm.append((1,(min(vals), max(vals))))\n",
        "                bnd.append(pts)\n",
        "    except ImportError:\n",
        "        print(\"Networkx not found, loops not computed\")\n",
        "\n",
        "    # branches\n",
        "    for topo_type in [\"downbranch\", \"upbranch\"]:\n",
        "\n",
        "        lfunction = []\n",
        "        for i in range(num_nodes):\n",
        "            lfunction.append(function[invnamefunc[i]])\n",
        "\n",
        "        # upranch is downbranch of opposite function\n",
        "        if topo_type == \"upbranch\":\n",
        "            lfunction = [-f for f in lfunction]\n",
        "\n",
        "        # sort vertices according to function values and compute inverse function\n",
        "        sorted_idxs = np.argsort(np.array(lfunction))\n",
        "        inv_sorted_idxs = np.zeros(num_nodes)\n",
        "        for i in range(num_nodes):\n",
        "            inv_sorted_idxs[sorted_idxs[i]] = i\n",
        "\n",
        "        # go through all vertices in ascending function order\n",
        "        persistence_diag, persistence_set, parents, visited = {}, {}, -np.ones(num_nodes, dtype=np.int32), {}\n",
        "        for i in range(num_nodes):\n",
        "\n",
        "            current_pt = sorted_idxs[i]\n",
        "            neighbors = np.ravel(np.argwhere(A[current_pt,:] == 1))\n",
        "            lower_neighbors = [n for n in neighbors if inv_sorted_idxs[n] <= i] if len(neighbors) > 0 else []\n",
        "\n",
        "            # no lower neighbors: current point is a local minimum\n",
        "            if lower_neighbors == []:\n",
        "                parents[current_pt] = current_pt\n",
        "\n",
        "            # some lower neighbors exist\n",
        "            else:\n",
        "\n",
        "                # find parent pg of lower neighbors with lowest function value\n",
        "                neigh_parents = [find(n, parents) for n in lower_neighbors]\n",
        "                pg = neigh_parents[np.argmin([lfunction[n] for n in neigh_parents])]\n",
        "\n",
        "                # set parent of current point to pg\n",
        "                parents[current_pt] = pg\n",
        "\n",
        "                # for each lower neighbor, we will create a persistence diagram point and corresponding set of nodes\n",
        "                for neighbor in lower_neighbors:\n",
        "\n",
        "                    # get parent pn\n",
        "                    pn = find(neighbor, parents)\n",
        "                    val = lfunction[pn]\n",
        "                    persistence_set[pn] = []\n",
        "\n",
        "                    # we will create persistence set only if parent pn is not local minimum pg\n",
        "                    if pn != pg:\n",
        "                        # go through all strictly lower nodes with parent pn\n",
        "                        for v in sorted_idxs[:i]:\n",
        "                            if find(v, parents) == pn:\n",
        "                                # if it is already part of another persistence set, continue\n",
        "                                try:\n",
        "                                    visited[v]\n",
        "                                # else, mark visited and include it in current persistence set\n",
        "                                except KeyError:\n",
        "                                    visited[v] = True\n",
        "                                    persistence_set[pn].append(v)\n",
        "\n",
        "                        # add current point to persistence set\n",
        "                        persistence_set[pn].append(current_pt)\n",
        "\n",
        "                        # do union and create persistence point corresponding to persistence set if persistence is sufficiently large\n",
        "                        if np.abs(lfunction[pn]-lfunction[current_pt]) >= threshold:\n",
        "                            persistence_diag[pn] = current_pt\n",
        "                            union(pg, pn, parents, lfunction)\n",
        "\n",
        "        for key, val in iter(persistence_diag.items()):\n",
        "            if topo_type == \"downbranch\":\n",
        "                dgm.append((0, (lfunction[key],  lfunction[val])))\n",
        "            elif topo_type == \"upbranch\":\n",
        "                dgm.append((0, (-lfunction[val], -lfunction[key])))\n",
        "            bnd.append([invnamefunc[v] for v in persistence_set[key]])\n",
        "\n",
        "    bnd = [list(b) for b in bnd]\n",
        "    M.persistence_diagram, M.persistence_sets = dgm, bnd\n",
        "    return dgm, bnd\n",
        "\n",
        "def bootstrap_topological_features(M, N):\n",
        "    \"\"\"\n",
        "    Use bootstrap to empirically assess stability of the features. This function computes a distribution of\n",
        "    bottleneck distances, that can used afterwards to run tests on each topological feature.\n",
        "    Parameters:\n",
        "        N (int): number of bootstrap iterations.\n",
        "    \"\"\"\n",
        "\n",
        "    dgm = M.persistence_diagram\n",
        "    num_pts, distribution = len(M.data), []\n",
        "    for bootstrap_id in range(N):\n",
        "\n",
        "        print(str(bootstrap_id) + \"th iteration\")\n",
        "\n",
        "        # Randomly select points\n",
        "        idxs = np.random.choice(num_pts, size=num_pts, replace=True)\n",
        "        Xboot = M.data[idxs,:] if M.input_type == \"point cloud\" else M.data[idxs,:][:,idxs]\n",
        "        f_boot, c_boot = M.filters[idxs,:], M.colors[idxs,:]\n",
        "        Mboot = M.__class__(filters=f_boot, filter_bnds=M.filter_bnds, colors=c_boot,\n",
        "                            resolutions=M.resolutions, gains=M.gains,\n",
        "                            input_type=M.input_type, clustering=M.clustering).fit(Xboot)\n",
        "\n",
        "        # Compute the corresponding persistence diagrams\n",
        "        dgm_boot, _ = compute_topological_features(Mboot)\n",
        "\n",
        "        # Compute the bottleneck distance\n",
        "        npts, npts_boot = len(dgm), len(dgm_boot)\n",
        "        D1 = np.array([[dgm[pt][1][0], dgm[pt][1][1]] for pt in range(npts)])\n",
        "        D2 = np.array([[dgm_boot[pt][1][0], dgm_boot[pt][1][1]] for pt in range(npts_boot)])\n",
        "        bottle = bottleneck_distance(D1, D2)\n",
        "        distribution.append(bottle)\n",
        "        M.distribution = np.sort(distribution)\n",
        "\n",
        "def get_distance_from_confidence_level(M, alpha=.95, complex_type='mapper'):\n",
        "    \"\"\"\n",
        "    Compute the bottleneck distance threshold corresponding to a specific confidence level.\n",
        "    Parameters:\n",
        "        alpha (float): confidence level.\n",
        "    Returns:\n",
        "        distance value (float); each feature whose size is above this distance is sure at confidence level alpha.\n",
        "    \"\"\"\n",
        "    return M.distribution[int(alpha*len(M.distribution))]\n",
        "\n",
        "def get_confidence_level_from_distance(M, distance):\n",
        "    \"\"\"\n",
        "    Compute the confidence level of a specific bottleneck distance threshold.\n",
        "    Parameters:\n",
        "        distance (float): bottleneck distance threshold.\n",
        "    Returns:\n",
        "        confidence level (float); each feature whose size is above the distance threshold is sure at\n",
        "        this confidence level.\n",
        "    \"\"\"\n",
        "    return len(np.argwhere(M.distribution <= distance))/len(M.distribution)\n",
        "\n",
        "def get_pvalue(M):\n",
        "    \"\"\"\n",
        "    Compute the p-value, i.e. the opposite of the confidence level of the largest bottleneck distance\n",
        "    preserving the topological features.\n",
        "    Returns:\n",
        "        p-value (float)\n",
        "    \"\"\"\n",
        "    distancemin = min([np.abs(pt[1][0]-pt[1][1]) for pt in M.persistence_diagram])\n",
        "    return 1.-M.compute_confidence_from_distance(distancemin)\n",
        "\n",
        "def compute_differential_coordinates(M, nodes=None, features=None, sparse=False):\n",
        "    \"\"\"\n",
        "    Compute the coordinates that best explain a set of nodes VS the rest of the nodes\n",
        "    (in the 1-skeleton of the cover complex) with a Kolmogorov-Smirnov test.\n",
        "    Only works if input_type is \"point cloud\".\n",
        "    Parameters:\n",
        "        nodes (list of integers): list of nodes to try. For instance, one can take the list of\n",
        "        nodes obtained after calling \"compute_topological_features\"\n",
        "        features (list of integers): the coordinates to try. All coordinates are tested if None.\n",
        "        sparse (bool): set to True if your data is sparse and there will be speedup, otherwise use False.\n",
        "    Returns:\n",
        "        features (list of integers): the list of coordinates, ranked from smallest to largest p-values.\n",
        "        p-values (list of float): the corresponding p-values.\n",
        "    \"\"\"\n",
        "    if M.input_type == \"distance matrix\":\n",
        "        print(\"Need coordinates for running differential coordinates!\")\n",
        "        raise\n",
        "    #coordinates_and_pvalues = compute_differential_coordinates(cover_complex, nodes=bnd_temp[i])\n",
        "\n",
        "    node_info = M.node_info\n",
        "    X = M.data\n",
        "    nodes = [s[0] for s,_ in self.simplex_tree.get_skeleton(0)] if nodes is None else nodes\n",
        "\n",
        "    if features is None:\n",
        "        features = np.arange(X.shape[1])\n",
        "\n",
        "    #The data points create this communities\n",
        "    list_idxs1 = list(np.unique(np.concatenate([node_info[node_name][\"indices\"] for node_name in nodes])))\n",
        "\n",
        "    #Other data points that do not create this community\n",
        "    list_idxs2 = list(set(np.arange(X.shape[0]))-set(list_idxs1))\n",
        "\n",
        "    pvals = []\n",
        "    ttvals = []\n",
        "    tpvals = []\n",
        "    for f in features:\n",
        "        if sparse:\n",
        "            Xsp = csr_matrix(X)\n",
        "            group1 = np.squeeze(np.array(Xsp[list_idxs1,f].todense()))\n",
        "            group2 = np.squeeze(np.array(Xsp[list_idxs2,f].todense()))\n",
        "        else:\n",
        "            group1, group2 = X[list_idxs1,f], X[list_idxs2,f]\n",
        "\n",
        "        _,pval = ks_2samp(group1, group2)\n",
        "\n",
        "        #ttval = ttest_ind(group1, group2)[0]\n",
        "        #tpval = ttest_ind(group1, group2)[1]\n",
        "\n",
        "        pvals.append(pval)\n",
        "\n",
        "        #tpvals.append(tpval)\n",
        "        #ttvals.append(ttval)\n",
        "\n",
        "    pvals = np.array(pvals)\n",
        "\n",
        "    #tpvals= np.array(tpvals)\n",
        "    #ttvals= np.array(ttvals)\n",
        "\n",
        "    F_ks, P_ks = features[np.argsort(pvals)], np.sort(pvals)\n",
        "    #F_ent, P_ent = features[np.argsort(entvals)], np.sort(entvals)\n",
        "    return F_ks, P_ks #, np.argsort(ttvals), np.sort(ttvals)"
      ],
      "metadata": {
        "id": "qIMWHkKmfQ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 Define filters for Mapper"
      ],
      "metadata": {
        "id": "YLvwDI97IAvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "from gudhi import SimplexTree, CoverComplex\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
        "import torch\n",
        "import scipy.sparse\n",
        "import torch\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.testing import withPackage\n",
        "from torch_geometric.utils import (\n",
        "    from_networkx,\n",
        "    from_scipy_sparse_matrix,\n",
        "    from_trimesh,\n",
        "    subgraph,\n",
        "    to_networkx,\n",
        "    to_scipy_sparse_matrix,\n",
        "    to_trimesh,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Topological_torch_geometric_graphs=[]\n",
        "\n",
        "\n",
        "for dti in range( 0,15):#len(DTI_sym)):\n",
        "  print(dti)\n",
        "\n",
        "  DTI_sym_np = DTI_sym[dti].to_numpy()\n",
        "\n",
        "  #Calculate L-Infinity filter:\n",
        "  pairwise_dist = squareform(pdist(DTI_sym_np, 'euclidean'))\n",
        "  L_infinity = np.amax(pairwise_dist, axis = 1)\n",
        "  #print(L_infinity.shape)\n",
        "\n",
        "  svd = TruncatedSVD(n_components=1, n_iter=7, random_state=42)\n",
        "  SVD_filter = svd.fit_transform(DTI_sym_np)\n",
        "  #print(SVD_filter.shape)\n",
        "\n",
        "  filters= np.concatenate((L_infinity[:,np.newaxis], SVD_filter ), axis=1)\n",
        "\n",
        "  #DEFINE MAPPER for each graph instance\n",
        "  cover_complex = MapperComplex(\n",
        "    input_type='distance matrix',\n",
        "    colors = filters,\n",
        "    mask=0,\n",
        "\n",
        "    #clustering = None,\n",
        "    clustering=DBSCAN(eps = 0.5, min_samples=5, metric =\"cosine\"),\n",
        "    #clustering = AgglomerativeClustering(n_clusters=None, linkage=\"complete\",\n",
        "                                         #distance_threshold=0.5, affinity=\"cosine\"),\n",
        "\n",
        "    N=100, beta=0., C=10,\n",
        "\n",
        "    #LENS:\n",
        "    filters = filters,\n",
        "\n",
        "    filter_bnds=None,\n",
        "\n",
        "    #resolutions = Number of intervals required to cover each filter image\n",
        "    #When you increase this, num of communities increase\n",
        "    resolutions = np.array([13, 13]), # When you decrease this, num of data points decrease. 90\n",
        "\n",
        "    #gains = Overlap percentage of the intervals covering each filter image.\n",
        "    gains=np.array([0.5, 0.5]),  #When you decrease this, data-points lose connections 0.91\n",
        "    input_name=\"human\", cover_name=\"coord2\", color_name=\"coord2\", verbose=True)\n",
        "  #MAPPER COMPLEX COMPUTATION\n",
        "  _ = cover_complex.fit(DTI_sym_np)\n",
        "  G = cover_complex.get_networkx()\n",
        "\n",
        "\n",
        "  #Node attribute'ü ayarla:\n",
        "  eigen = nx.eigenvector_centrality(G, weight='weight',tol=1e-03)\n",
        "  #betweenness = nx.betweenness_centrality(G, weight='distance', normalized=True)\n",
        "  #closeness = nx.closeness_centrality(G)\n",
        "  node_attributes=[]\n",
        "  for i in range(len(eigen)):\n",
        "    node_attributes.append( [ eigen[i]] ) #, betweenness[i], closeness[i]] )\n",
        "  x = torch.tensor(node_attributes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Edge index'i ayarla\n",
        "  pyg_graph = from_networkx(G)\n",
        "  #edge_index = pyg_graph.edge_index\n",
        "\n",
        "  index =0\n",
        "  already_looked=[(0,0)]\n",
        "  edge_index_tmp = torch.empty((2, int(pyg_graph.edge_index.shape[1]/2)))\n",
        "  for i in range( int(pyg_graph.edge_index.shape[1]) ):\n",
        "    if is_in(pyg_graph.edge_index[0][i], pyg_graph.edge_index[1][i], already_looked) == 0:\n",
        "      edge_index_tmp[0][index] = int(pyg_graph.edge_index[0][i])\n",
        "      edge_index_tmp[1][index] = int(pyg_graph.edge_index[1][i])\n",
        "      already_looked.append( (pyg_graph.edge_index[1][i], pyg_graph.edge_index[0][i]) )\n",
        "      already_looked.append( (pyg_graph.edge_index[0][i], pyg_graph.edge_index[1][i]) )\n",
        "      index +=1\n",
        "  print(\"edge_index = \",edge_index_tmp)\n",
        "  print(len(edge_index_tmp), len(edge_index_tmp[0]))\n",
        "\n",
        "\n",
        "  bb = nx.edge_betweenness_centrality(G, normalized=False)\n",
        "  print(bb)\n",
        "  nx.set_edge_attributes(G, bb, \"betweenness\")\n",
        "\n",
        "  #Edge attribute'ü ayarla\n",
        "\n",
        "  edge_attr0=[]\n",
        "  for (key, value)in bb.items():\n",
        "    edge_attr0.append(value)\n",
        "    #edge_attr0.append([value])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print()\n",
        "  print(edge_attr0)\n",
        "  print()\n",
        "  #edge_attr = [float(i)/sum(edge_attr0) for i in edge_attr0]\n",
        "\n",
        "\n",
        "\n",
        "  #Data objesini hazırla\n",
        "  data = Data(x=x,  edge_index=edge_index_tmp.long(), edge_attr=torch.Tensor(edge_attr0), y = torch.LongTensor([ages[dti]]) )\n",
        "\n",
        "\n",
        "  Topological_torch_geometric_graphs.append(data)\n",
        "\n",
        "Topological_torch_geometric_graphs[0]"
      ],
      "metadata": {
        "id": "Mua1lLUSmBAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Topological_torch_geometric_graphs[0]"
      ],
      "metadata": {
        "id": "HejNAb2qMV-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Topological_torch_geometric_graphs[0]\n"
      ],
      "metadata": {
        "id": "103-4BAxmBEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTI_torch_geometric_graphs[0]"
      ],
      "metadata": {
        "id": "hPtIPkN9mBGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FzdI1z4mBI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. GNN model (raw + topological graphs)"
      ],
      "metadata": {
        "id": "hHXHFEo22csZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DTI_torch_geometric_graphs + Topological_torch_geometric_graphs[:8]\n",
        "test_dataset = Topological_torch_geometric_graphs[8:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "id": "5AefwcRa2gyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruA50JTl4PA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "jQOCuLt4x4M1"
      },
      "source": [
        "from torch.nn import Linear, Softmax\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, TransformerConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GNN2(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_channels, conv, conv_params={}):\n",
        "        super(GNN2, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = conv(\n",
        "            input_size, hidden_channels, **conv_params)\n",
        "        self.conv2 = conv(\n",
        "            hidden_channels, hidden_channels, **conv_params)\n",
        "        self.lin = Linear(hidden_channels, 4)\n",
        "\n",
        "    def forward(self, x, edge_index, batch,  edge_col = None):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index, edge_col)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index, edge_col)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        batch = torch.zeros(data.x.shape[0],dtype=int) if batch is None else batch\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = GNN2(input_size = 1, hidden_channels=64, conv = GCNConv)\n",
        "print(model)"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0,
          19,
          37,
          73
        ],
        "id": "92GJTgULx4M2"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "def train(train_loader, model, optimizer, criterion, edge_col_name):\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    #print(\"edge_col_name = \",edge_col_name)\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        #print(data)\n",
        "        if edge_col_name==None:\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "        else:\n",
        "            #print(data)\n",
        "            out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
        "\n",
        "        loss = criterion(out, data.y.long())  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(loader, model,  edge_col_name, metric = metrics.accuracy_score):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    score = 0\n",
        "    for data in loader:\n",
        "        if edge_col_name==None:\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "        else:\n",
        "            out = model(data.x, data.edge_index, data.batch, data[edge_col_name])\n",
        "\n",
        "        y_pred = out.argmax(dim=1).detach().numpy()\n",
        "        y_true = data.y.long().cpu().detach().numpy()\n",
        "\n",
        "        score += metric(y_true, y_pred)\n",
        "\n",
        "    return score/len(loader)\n",
        "\n",
        "def train_test(\n",
        "    train_loader, test_loader, model, model_params,   edge_col_name=None,\n",
        "    learning_rate=0.01, e_patience = 10, min_acc= 0.005, n_epochs=500):\n",
        "\n",
        "\n",
        "    model = model(**model_params)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    k=0\n",
        "\n",
        "    loss, train_score, test_score = [], [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss += [train(train_loader, model,  optimizer, criterion, edge_col_name)]\n",
        "        train_score += [test(train_loader, model, edge_col_name)]\n",
        "        test_score += [test(test_loader, model, edge_col_name)]\n",
        "\n",
        "        if (epoch+1)%10==0:\n",
        "            print(f'Epoch: {epoch+1:03d}, Loss: {loss[-1]:.4f}, Train: {train_score[-1]:.4f}, Test: {test_score[-1]:.4f}')\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            'loss': loss,\n",
        "            'train_score': train_score, 'test_score': test_score,\n",
        "            #'time':(time.time()-t0)/60\n",
        "        })\n",
        "\n",
        "        # enable early stopping\n",
        "        if (epoch > 1) and abs(loss[-1]/loss[-2]-1) < min_acc :\n",
        "            k += 1\n",
        "        if k> e_patience:\n",
        "            print('Early stopping, epoch', epoch)\n",
        "            break\n",
        "\n",
        "    return model, results\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "id": "UvIQWsY_x4M2"
      },
      "source": [
        "N_EPOCHS = 500\n",
        "E_PATIENCE = 50\n",
        "LEARNING_RATE = 0.01"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "id": "zZX7GCuOx4M2"
      },
      "source": [
        "N = 3"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "TJzvQ721x4M3"
      },
      "source": [
        "def train_test_CV(train_dataset, test_dataset, GNN, model_params,\n",
        "                  edge_col_name = None,\n",
        "                  learning_rate=0.01, e_patience = 10, min_acc= 0.005, n_epochs=500, N=4):\n",
        "\n",
        "    for i in range(N):\n",
        "        print(\"CROSS VALIDATION - \", i+1)\n",
        "\n",
        "        #train_dataset, test_dataset = split_dataset(dataset_target, Y)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "        #print(\"geldi0\")\n",
        "        for data in train_loader:\n",
        "          #print(data)\n",
        "\n",
        "          model, results = train_test(\n",
        "              train_loader, test_loader,\n",
        "              GNN, model_params,   edge_col_name,\n",
        "              learning_rate, e_patience, n_epochs = n_epochs)\n",
        "\n",
        "          results_df = results.iloc[-1:] if i == 0 else pd.concat([results_df, results.iloc[-1:]])\n",
        "\n",
        "          #print(results_df)\n",
        "\n",
        "    means = pd.DataFrame(results_df.mean()).T.rename(columns={c:c+'_mean' for c in results_df.columns})\n",
        "    stds = pd.DataFrame(results_df.std()).T.rename(columns={c:c+'_std' for c in results_df.columns})\n",
        "    results_df = pd.concat([means, stds], axis=1).reset_index(drop=True)\n",
        "\n",
        "    return model, results_df"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dZRnfRGLx4M4"
      },
      "source": [
        "# using edge_weight for GCN\n",
        "\n",
        "model_params = {\"input_size\":1, \"hidden_channels\":64, 'conv':GCNConv}\n",
        "\n",
        "model, results = train_test_CV(\n",
        "    train_dataset, test_dataset, GNN2, model_params,\n",
        "      edge_col_name='edge_attr',\n",
        "    learning_rate=LEARNING_RATE, e_patience = E_PATIENCE, n_epochs=N_EPOCHS)\n",
        "\n",
        "results['model'] = 'GCNConv'\n",
        "results['nb_layers'] = 3\n",
        "results['edge_weight'] = True\n",
        "#results_df['edge_weight'] = False\n",
        "\n",
        "#results_df = pd.concat([results_df, results], axis=0)"
      ],
      "outputs": []
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "id": "SW6nbljQx4M4"
      },
      "source": [
        "y_pred, y_true = [], []\n",
        "\n",
        "model.eval()\n",
        "for data in test_dataset:\n",
        "    out =  model(data.x, data.edge_index, batch=None)\n",
        "    y_pred += [out.argmax(dim=1).detach().numpy()]\n",
        "    y_true += [data.y.long().cpu().detach().numpy()]\n",
        "\n",
        "print(metrics.classification_report(y_true, y_pred))\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oERXvqeMLngW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}